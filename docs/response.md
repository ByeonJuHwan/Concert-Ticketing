# 장애대응

콘서트 좌석 예약의 경우 부하테스트 결과 6,0000명 이상의 부하가 몰릴시 장애로 이어집니다.

따라서 6,000명 이상의 부하가 발생했을시 어떻게 해결하고 대응해야하는지 가상 장애 대응을 진행해 보겠습니다.

## 장애 모니터링 및 알림

Grafana 에서 현재 시스템을 지속적으로 모니터링을 진행하며, cpu 사용량 80 % 초과시
담당 개발자가 인지 할 수 있도록 Slack 알림을 보냅니다.

## 원인 분석 및 조치

- 데이터베이스 연결 풀이 포화 상태임을 확인
- 애플리케이션 서버의 스레드 풀도 거의 소진된 상태 확인

1. 데이터베이스 연결 풀 확장
    - 최대 연결수를 200 -> 400으로 증가
    - HikariCP 설정 조정
2. 애플리케이션 서버 스케일 아웃
   - Kubernetes 클러스터에 3개의 새로운 파드 추가
3. 캐시 레이어 강화
    - Redis 캐시 크기 2배 확장
    - 자주 조회되는 콘서트 정보 캐시 적용

안정화 및 모니터링

## 사후 분석 및 개선계획

1. 자동 스케일링 정책 수립
2. 데이터베이스 쿼리 최적화
3. 부하 테스트 시나리오 강화
4. 긴급 상황 대응 메뉴얼 업데이트

